df$Caesarian <- as.factor(df$Caesarian)
summary(df)
str(df)
library(ggplot2)
par(mfrow=c(2,3))
for (i in 1:dim(df)[2]) {
hist(as.integer(df[,i]), main = paste(names(df[i]), "histogram"), col = "blue")
}
c_cut <- df$Caesarian
for (i in 1:5) {
pairs(df[, c(i, 6)], col=c("red", "blue")[c_cut], pch=c(4, 8)[c_cut])
}
df[df$DeliveryNumber == 4, ]
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
print(mean(accuracy))
#Using LLOCV we get 61.25% accuracy of our model.
#10 K-Fold
kfold.model = glm(Caesarian ~., data = df, family = "binomial")
kfold.cv = cv.glm(df, kfold.model, K = 10)
library(tree)
tree.model <- tree::tree(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, df)
summary(tree.model)
plot(tree.model)
text(tree.model, pretty=0)
# Calculating model accuracy
set.seed(700)
train <- sample(1:nrow(df), nrow(df)/2)
df.test <- df[-train,]
df.response <- df$Caesarian[-train]
df.train.tree <- tree(Caesarian ~ ., df, subset=train)
plot(df.train.tree)
text(df.train.tree,pretty=0)
df.pred <- predict(df.train.tree, df.test, type="class")
table(df.pred, df.response)
print(1 - mean(df.pred!= df.response))
# With decision tree accuracy is 65%.
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
print(mean(accuracy))
#Using LLOCV we get 61.25% accuracy of our model.
#10 K-Fold
library(boot)
kfold.model = glm(Caesarian ~., data = df, family = "binomial")
kfold.cv = cv.glm(df, kfold.model, K = 10)
print(1 - kfold.cv$delta[1])
#Using 10 K-Fold CV we get 75.3% accuracy of our model.
library(tree)
tree.model <- tree::tree(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, df)
summary(tree.model)
plot(tree.model)
text(tree.model, pretty=0)
# Calculating model accuracy
set.seed(700)
train <- sample(1:nrow(df), nrow(df)/2)
df.test <- df[-train,]
df.response <- df$Caesarian[-train]
df.train.tree <- tree(Caesarian ~ ., df, subset=train)
plot(df.train.tree)
text(df.train.tree,pretty=0)
df.pred <- predict(df.train.tree, df.test, type="class")
table(df.pred, df.response)
print(1 - mean(df.pred!= df.response))
# With decision tree accuracy is 65%.
install.packages("randomForest")
#Decision Tree
library(tree)
tree.model <- tree::tree(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, df)
summary(tree.model)
plot(tree.model)
text(tree.model, pretty=0)
# Calculating model accuracy
set.seed(700)
train <- sample(1:nrow(df), nrow(df)/2)
df.test <- df[-train,]
df.response <- df$Caesarian[-train]
df.train.tree <- tree(Caesarian ~ ., df, subset=train)
plot(df.train.tree)
text(df.train.tree,pretty=0)
df.pred <- predict(df.train.tree, df.test, type="class")
table(df.pred, df.response)
print(1 - mean(df.pred!= df.response))
# With decision tree accuracy is 65%.
#Random Forests
library(randomForest)
rf_train <- sample(df, nrow(df)/2)
?sample
rf_train <- sample(df, nrow(df)/2)
rf_train <- sample(1:nrow(df), nrow(df)/2)
rf_train
set.seed(700)
rf_train <- sample(1:nrow(df), nrow(df)/2)
rf_train
len(rf_train)
length(rf_train)
df
df[1, ]
df[, c(1,2,3) ]
rf_train <- df[indices, ]
set.seed(700)
indices <- sample(1:nrow(df), nrow(df)/2)
rf_train <- df[indices, ]
rf_train
library(randomForest)
set.seed(700)
indices <- sample(1:nrow(df), nrow(df)/2)
rf_train <- df[indices, ]
rft_test <- df[-indices, ]
rft_test
rf_train
#Decision Tree
library(tree)
tree.model <- tree::tree(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, df)
summary(tree.model)
plot(tree.model)
text(tree.model, pretty=0)
# Calculating model accuracy
set.seed(700)
train <- sample(1:nrow(df), nrow(df)/2)
df.test <- df[-train,]
df.response <- df$Caesarian[-train]
df.train.tree <- tree(Caesarian ~ ., df, subset=train)
plot(df.train.tree)
text(df.train.tree,pretty=0)
df.pred <- predict(df.train.tree, df.test, type="class")
table(df.pred, df.response)
print(1 - mean(df.pred!= df.response))
# With decision tree accuracy is 65%.
#Random Forests
library(randomForest)
set.seed(700)
indices <- sample(1:nrow(df), nrow(df)/2)
rf_train <- df[indices, ]
rf_test <- df[-indices, ]
rf_model <- randomForest(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, df)
summary(rf_model)
rf_model <- randomForest(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, data = df, subset = rf_train, importance = TRUE)
rf_model <- randomForest(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, data = df, subset = indices, importance = TRUE)
summary(rf_model)
#mtry = sqrt(p), p = number of predictors (5), mtry = 3
rf_model <- randomForest(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, data = df, subset = indices, importance = TRUE, mtry = 3)
#Decision Tree
library(tree)
tree.model <- tree::tree(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, df)
summary(tree.model)
plot(tree.model)
text(tree.model, pretty=0)
# Calculating model accuracy
set.seed(700)
train <- sample(1:nrow(df), nrow(df)/2)
df.test <- df[-train,]
df.response <- df$Caesarian[-train]
df.train.tree <- tree(Caesarian ~ ., df, subset=train)
plot(df.train.tree)
text(df.train.tree,pretty=0)
df.pred <- predict(df.train.tree, df.test, type="class")
table(df.pred, df.response)
print(1 - mean(df.pred!= df.response))
# With decision tree accuracy is 65%.
#Random Forests
library(randomForest)
set.seed(700)
indices <- sample(1:nrow(df), nrow(df)/2)
rf_train <- df[indices, ]
rf_test <- df[-indices, ]
#mtry = sqrt(p), p = number of predictors (5), mtry = 3
rf_model <- randomForest(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, data = df, subset = indices, importance = TRUE, mtry = 3)
summary(rf_model)
rf_pred <- predict(rf_model, newdata = rf_test)
rf_pred
print(table(rf_pred, test$Caesarian))
length(rf_pred)
len(test$Caesarian)
length(test$Caesarian)
test$Caesarian
test
rf_test
rf_pred <- predict(rf_model, newdata = rf_test)
print(table(rf_pred, rf_test$Caesarian))
print(1- mean(rf_pred != rf_test$Caesarian))
#Decision Tree
library(tree)
tree.model <- tree::tree(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, df)
summary(tree.model)
plot(tree.model)
text(tree.model, pretty=0)
# Calculating model accuracy
set.seed(700)
train <- sample(1:nrow(df), nrow(df)/2)
df.test <- df[-train,]
df.response <- df$Caesarian[-train]
df.train.tree <- tree(Caesarian ~ ., df, subset=train)
plot(df.train.tree)
text(df.train.tree,pretty=0)
df.pred <- predict(df.train.tree, df.test, type="class")
table(df.pred, df.response)
print(1 - mean(df.pred!= df.response))
# With decision tree accuracy is 65%.
#Random Forests
library(randomForest)
set.seed(700)
indices <- sample(1:nrow(df), nrow(df)/2)
rf_train <- df[indices, ]
rf_test <- df[-indices, ]
#mtry = sqrt(p), p = number of predictors (5), mtry = 3
rf_model <- randomForest(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, data = df, subset = indices, importance = TRUE, mtry = 3)
summary(rf_model)
rf_pred <- predict(rf_model, newdata = rf_test)
table(rf_pred, rf_test$Caesarian)
print(1- mean(rf_pred != rf_test$Caesarian))
# Using Random Forests we get accuracy of 62.5%.
df = read.csv("caesarian.csv.arff", skip = 15, header = FALSE)
colnames(df) <- c("Age", "DeliveryNumber", "DeliveryTime", "BloodPressure", "HeartProblem", "Caesarian")
df$DeliveryNumber <- as.factor(df$DeliveryNumber)
df$DeliveryTime <- as.factor(df$DeliveryTime)
df$BloodPressure <- as.factor(df$BloodPressure)
df$HeartProblem <- as.factor(df$HeartProblem)
df$Caesarian <- as.factor(df$Caesarian)
summary(df)
str(df)
table(log.pred, df$Caesarian)
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
print(mean(accuracy))
#Using LLOCV we get 61.25% accuracy of our model.
#10 K-Fold
library(boot)
kfold.model = glm(Caesarian ~., data = df, family = "binomial")
kfold.cv = cv.glm(df, kfold.model, K = 10)
print(1 - kfold.cv$delta[1])
#Using 10 K-Fold CV we get 76% accuracy of our model.
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian, row.names(c("True, Predicted")))
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian, row.names(c("True", "Predicted")))
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian()
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
print(mean(accuracy))
#Using LLOCV we get 61.25% accuracy of our model.
#10 K-Fold
library(boot)
kfold.model = glm(Caesarian ~., data = df, family = "binomial")
kfold.cv = cv.glm(df, kfold.model, K = 10)
print(1 - kfold.cv$delta[1])
#Using 10 K-Fold CV we get 76% accuracy of our model.
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
print("LOOCV", mean(accuracy))
#Using LLOCV we get 61.25% accuracy of our model.
#10 K-Fold
library(boot)
kfold.model = glm(Caesarian ~., data = df, family = "binomial")
kfold.cv = cv.glm(df, kfold.model, K = 10)
print(1 - kfold.cv$delta[1])
#Using 10 K-Fold CV we get 76% accuracy of our model.
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
print("LOOCV" + mean(accuracy))
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
print("LOOCV" + as.character(mean(accuracy)))
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
print("LOOCV", as.character(mean(accuracy)))
#Using LLOCV we get 61.25% accuracy of our model.
#10 K-Fold
library(boot)
kfold.model = glm(Caesarian ~., data = df, family = "binomial")
kfold.cv = cv.glm(df, kfold.model, K = 10)
print(1 - kfold.cv$delta[1])
#Using 10 K-Fold CV we get 76% accuracy of our model.
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
paste("LOOCV", mean(accuracy))
#Using LLOCV we get 61.25% accuracy of our model.
#10 K-Fold
library(boot)
kfold.model = glm(Caesarian ~., data = df, family = "binomial")
kfold.cv = cv.glm(df, kfold.model, K = 10)
print(1 - kfold.cv$delta[1])
#Using 10 K-Fold CV we get 76% accuracy of our model.
log.model <- glm(Caesarian ~., data = df, family = "binomial")
summary(log.model)
# Delivery time, Blood pressure and Heart problem are statistically significant.
log.pred <- predict(log.model, type = "response")
log.pred[log.pred < 0.5] <- 0
log.pred[log.pred >= 0.5] <- 1
table(log.pred, df$Caesarian)
# Making prediction using our model we got 71% observations correctly classified.
#LOOCV
accuracy = NULL
for (i in 1:nrow(df)) {
train <- df[-i, ]
test <- df[i, ]
log.model <- glm(Caesarian ~., data = train, family = "binomial")
log.pred <- predict(log.model, subset(test[, 1:5]), type = "response")
pred <- ifelse(log.pred > 0.5,1,0)
error <- mean(pred != test$Caesarian)
accuracy[i] <- 1-error
}
paste("LOOCV accuracy is", mean(accuracy))
#Using LLOCV we get 61.25% accuracy of our model.
#10 K-Fold
library(boot)
kfold.model = glm(Caesarian ~., data = df, family = "binomial")
kfold.cv = cv.glm(df, kfold.model, K = 10)
paste("10 fold CV accuracy is", (1 - kfold.cv$delta[1]))
#Using 10 K-Fold CV we get 76% accuracy of our model.
#Decision Tree
library(tree)
tree.model <- tree::tree(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, df)
summary(tree.model)
plot(tree.model)
text(tree.model, pretty=0)
# Calculating model accuracy
set.seed(700)
train <- sample(1:nrow(df), nrow(df)/2)
df.test <- df[-train,]
df.response <- df$Caesarian[-train]
df.train.tree <- tree(Caesarian ~ ., df, subset=train)
#plot(df.train.tree)
#text(df.train.tree,pretty=0)
df.pred <- predict(df.train.tree, df.test, type="class")
table(df.pred, df.response)
paste("Using decision trees accuracy is", (1 - mean(df.pred!= df.response)))
# With decision tree accuracy is 65%.
#Random Forests
library(randomForest)
set.seed(700)
indices <- sample(1:nrow(df), nrow(df)/2)
rf_train <- df[indices, ]
rf_test <- df[-indices, ]
#mtry = sqrt(p), p = number of predictors (5), mtry = 3
rf_model <- randomForest(Caesarian ~ Age+DeliveryNumber+DeliveryTime+BloodPressure+HeartProblem, data = df, subset = indices, importance = TRUE, mtry = 3)
summary(rf_model)
rf_pred <- predict(rf_model, newdata = rf_test)
table(rf_pred, rf_test$Caesarian)
paste("Using Random Forests accuracy is",(1- mean(rf_pred != rf_test$Caesarian)))
# Using Random Forests we get accuracy of 62.5%.
